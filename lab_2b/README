NAME: Nathan Tsai
EMAIL: nwtsai@gmail.com
ID: 304575323

Included Files:
lab2_list.c --> source code, updating a shared linked list, allowing users to customize the number of threads, number of iterations, number of lists, yielding options, and syncing options. In terms of syncing options, the user can choose to not lock, use mutexes, or use spin locks. In terms of yielding options, the user can specify insert yiedling, delete yielding, or look up yielding. 
Makefile --> uses the following targets: default (compile programs), tests (run all test cases to generate CSV files), graphs (run gnuplot scripts to generate graphs that analyze run time data), profile (create an execution profiling report using a profiling tool), dist (tarballs files for submission), and clean (removes files that Makefile creates)
README --> documents name, email, id, included files, answers to questions, and research links
lab2b_1.png --> throughput vs number of threads for mutex and spin-lock synchronized list operations
lab2b_2.png --> mean time per mutex wait and mean time per operation for mutex-synchronized list operations
lab2b_3.png --> successful iterations vs threads for each synchronization method
lab2b_4.png --> throughput vs number of threads for mutex synchronized partitioned lists
lab2b_5.png --> throughput vs number of threads for spin-lock-synchronized partitioned lists
SortedList.h --> header interface file provided by specifications that details how to implement the functions SortedList_insert, SortedList_delete, SortedList_lookup, SortedList_length
SortedList.c --> source code that implements the 4 functions specified by SortedList.h, taking into account yielding options and corrupted or empty linked lists
lab2b_list.csv --> CSV file generated by make tests (Makefile) which runs lab2_list, taking the formatted output from the program to prepare for creating gnuplot graphs
lab2b_list.gp --> gnuplot file given in the specifications to generate the PNG files. It referenecs the lab2_list.csv file and generates graphs from the csv file
profile.out --> output from using gperf tool, which reports the time spent in different parts of the code

Answers to Questions:

QUESTION 2.3.1 - Cycles in the basic list implementation:

Where do you believe most of the cycles are spent in the 1 and 2-thread list tests?

In the insert function for the 1 and 2-thread tests, most of the cycles are spent in the list operations, not in the locking operations.

Why do you believe these to be the most expensive parts of the code?

This is the most expensive part of the code because locking and unlocking take very little time. This is because the probability of two threads trying to access the same critical section is very low, so a thread rarely blocks when attempting to grab a mutex. Thus, most of the time will go to the insert, look up, and delete functionality of the lists. 

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?

In the high-thread spin-lock tests, most of the time/cycles are being spent spinning and checking spin-locks. When there are many threads, resource contention increases, which means that the probability of multiple threads trying to access the same critical section increases. If one thread has the spin-lock, then all other threads will spin until the spin-lock is released; thus, in the high-thread tests, most of the time/cycles are spent attempting to obtain the spin-lock.

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

In the high-thread mutex tests, if the length of the list is very long, then most of the CPU cycles will be spent on the list operations. If the length of the list is not very long, most of the time/cycles will be spent on context-switching between threads, because context-switching is extremely expensive, especially if there are a high number of threads, because this means that context switches happen more frequently.

QUESTION 2.3.2 - Execution Profiling:

Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

In the spin-lock version of the list exerciser, the while loop that spins until the spin lock is released is the part of the code that consumes most of the cycles. The code I am referencing is while(__sync_lock_test_and_set(&spin_lock, 1));
The gperftool tells me this.

Why does this operation become so expensive with large numbers of threads?

In the case of a large number of threads, there is a high chance that multiple threads will fight for one spin lock, so all threads that don't have the spin-lock are left spinning, consuming CPU cycles. Lock contention is much higher when there are a large number of threads.

QUESTION 2.3.3 - Mutex Wait Time:

Look at the average time per operation (vs # threads) and the average wait-for-mutex time (vs #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?

Because there are a large number of contending threads, there will be more threads competing for the lock, so the average lock-wait time rises dramatically. Only one thread can enter a critical section at a time, so the chance of a thread or multiple threads waiting on a lock increases significantly as the number of contending threads increases.

Why does the completion time per operation rise (less dramatically) with the number of contending threads?

The completion time per operation is slightly affected by the number of contending threads. Some overhead will be added when threads context switch; also, time for contention increases with a high number contending threads. Overall, the increasing number of threads affects the average lock-wait time more dramatically than the completion time per operation.

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The completion time accounts for the whole operation. The wait time per operation accounts for the time in each individual thread. Because threads have many overlapping wait times, wait time per operation increases at a much higher rate than completion time.

QUESTION 2.3.4 - Performance of Partitioned Lists

Explain the change in performance of the synchronized methods as a function of the number of lists.

As the number of sublists increases, the chance of multiple threads competing for a lock decreases, so there's less cycles wasted on a thread blocking or spinning. Thus, the performance will initially increase as the number of sublists increases. However, as the number of lists continues to increase, the number of mutexes increase, so the performance of a mutex test decreases, because the overhead of obtaining the locks increases, and the overhead of threads context-switching increases as well.

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

With the same reasoning, the throughput should not continue to increases as the number of lists is further increased, because eventually, there will be so many lists that the overhead of locking and unlocking becomes significant. If I grab a lock just to update one or a few elements of the list, and then release the lock, the frequency of locking or unlocking increases, which decreases the overall throughput. 
Furthermore, the throughput will depend on the CPU and the number of concurrent threads it can run. With enough lists, the chance of contention approaches 0, so the throughput will not continue to increase, but will approach some limit.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

No. Creating sublists from a list shortens the list and reduces the time spent in the critical sections. This means that the probability of resource contention decreases when a list is partitioned.

Research Links:
Learning and downloading gperftools: https://github.com/gperftools/gperftools/releases

NAME: Nathan Tsai
EMAIL: nwtsai@gmail.com
ID: 304575323

Included Files:
lab2_add.c --> source code for Part 1, updating a shared variable that allow users to customize number of threads, number of iterations, whether or not to implement yielding, and syncing options. In terms of syncing options, the user can choose to not lock, use mutexes, spin locking with test and set, or the atomic compare then swap option. 
lab2_list.c --> source code for Part 2, updating a shared linked list, allowing users to customize the number of threads, number of iterations, yielding options, and syncing options. In terms of syncing options, the user can choose to not lock, use mutexes, or use spin locks. In terms of yielding options, the user can specify insert yiedling, delete yielding, or look up yielding. 
Makefile --> uses the following targets: build (compile programs), tests (run all test cases to generate CSV files), graphs (run gnuplot scripts to generate graphs that analyze run time data), dist (tarballs files for submission), and clean (remove files that Makefile creates)
README --> documents name, email, id, included files, answers to questions, and research links
lab2_add-1.png --> threads and iterations required to generate a failure (with and without yields)
lab2_add-2.png --> average time per operation with and without yields
lab2_add-3.png --> average time per (single threaded) operation vs. the number of iterations
lad2_add-4.png --> threads and iterations that can run successfully with yields under each of the synchronization options
lab2_add-5.png --> average time per (protected) operation vs. the number of threads
lab2_list-1.png --> average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length)
lab2_list-2.png --> threads and iterations required to generate a failure (with and without yields)
lab2_list-3.png --> iterations that can run (protected) without failure
lab3_list-4.png --> (length-adjusted) cost per operation vs the number of threads for the various synchronization options
SortedList.h --> header interface file provided by specifications that details how to implement the functions SortedList_insert, SortedList_delete, SortedList_lookup, SortedList_length
SortedList.c --> source code that implements the 4 functions specified by SortedList.h, taking into account yielding options and corrupted or empty linked lists
lab2_add.csv --> CSV file generated by make tests (Makefile) which runs lab2_add, taking the formatted output from the program to prepare for creating gnuplot graphs
lab2_list.csv --> CSV file generated by make tests (Makefile) which runs lab2_list, taking the formatted output from the program to prepare for creating gnuplot graphs
lab2_add.gp --> gnuplot file given in the specifications to generate the PNG files. It references the lab2_add.csv file and generates graphs from the csv file
lab2_list.gp --> gnuplot file given in the specifications to generate the PNG files. It referenecs the lab2_list.csv file and generates graphs from the csv file

Answers to Questions:
QUESTION 2.1.1 - causing conflicts:

Why does it take many interations before errors are seen?

As the number of iterations increases, the chance of a race condition happening within the critical section increases. In order for failure to happen, the threads executing the addition (which breaks down into multiple machine language instructions) need to execute in a specific order for failure to happen, and the higher the number of iterations, the higher the chance that the specific machine language order will result in failure.

Why does a significantly smaller number of iterations so seldom fail?

In contrast, if the number of iterations is small, then the probability of an unfortunate ordering of machine language instructions is also small. Because of the non-deterministic nature of the multithreaded program, the outcome is variable; however, the outcome is more likely to be correct in this case because we have reduced the number of iterations, decreasing the chance that two or more threads update the shared variable at the exact same time.

QUESTION 2.1.2 - cost of yielding:

Why are the --yield runs so much slower? Where is the additional time going?

Yielding is when a thread gives up the CPU to another thread. However, the action of giving the CPU to another thread is called a context switch, where the current thread's state is saved in the TCB. The overhead lies in the context switch itself, as a preemptive thread scheduler saves the state of the thread and loads the next thread residing in a waiting queue.

Is it possible to get valid per-operation timings if we are using the --yield option? If so, explain how. If not, explain why not.

No, it is not possible to get valid per-operation timings with yield. The expensive operations involved with context switching and yielding also get calculated into the per-operation timings. The elapsed time purely counts the time that has passed from the beginning to the end of the computation, including the overhead from context switching. Thus, with the yield option, the calculation does not accurately reflect the true per-operation timings.

QUESTION 2.1.3 - measurement errors:

Why does the average cost per operation drop with increasing iterations?

The calculation of the average cost per operation consists of the cost of creating the threads, doing the computation, and the number of operations. Average cost per operation = elapsed time / number of operations. As the number of iterations increases, the number of operations increases at a higher rate than the increasing rate of the elapsed time. Thus, because the denominator of the calculation increases as the number of iterations increases, then we know that the average cost per operation drops as well.

If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?

When we take the limit of the cost per iteration by setting the number of iterations to infinity, we see that the cost per iteration converges and stabilizes to the time it takes for the addition operation, and nothing more. The overhead from creating the threads becomes irrelevant when the number of iterations approaches infinity; thus, the pure cost of the adding operation is essentially the "correct" cost. 

QUESTION 2.1.4 - costs of serialization:

Why do all of the options perform similarly for low numbers of threads?

The chance of a thread actually needing to wait for the lock is quite low when dealing with a low number of threads. The instructions essentially run sequentially because very few threads actually need to wait, and if they do, the critical section is short enough that the time isn't significant. Furthermore, multiple CPUs along with a low number of threads essentially give each thread its own CPU, so no thread will need to wait on a lock, defauling to another free CPU. The resource contention is quite low because there aren't very many threads trying to update the shared variable at the same time, so the timing is similar for all options when there are only a few threads active.

Why do the three protected operations slow down as the number of threads rises?

In contrast, as the number of threads rises, resource contention increases, possibly creating a single-filed line for all threads trying to obtain a lock. When a thread attempts to grab a lock but the lock is already held, it waits until the lock is free. Because there are many threads, the chance of trying to enter a critical section when another thread is operating in the critical section is quite high. Thus, the operations slow down because of the added overhead of the idle threads that simply must wait for the lock. 

QUESTION 2.2.1 - scalability of Mutex

Compare the variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists). Comment on the general shapes of the curves, and explain why they have this shape. Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

The critical section of updating the linked list in Part-2 is much larger than the critical section of the updating the shared counter. Thus, because we are using one lock to insert an element and one lock to lookup and delete an element from the linked list, the cost of updating the shared linked list is much higher than the cost of updating the shared counter. Because the critical section is larger, the time that each thread holds the lock is much longer, thus increasing overhead for updating the shared linked list. Thus, the probability of many threads attempting to grab the same lock to enter the same critical section is much higher, reducing the amount of parallelism that can be achieved. 

For adding to a shared counter, the time per mutex-protected operation initially increases until there are 4 threads; however, the time per mutex-protected operation decreases when there are more than 4 threads. The shape is thus an upside down V, with the peak being at 4 threads. For updating a shared linked list, the time per mutex-protected operation increases as the number of threads increases. The shape is a line with a positive slope, as increasing the number of threads does not decrease the time per mutex-protected operation. 

QUESTION 2.2.2 - scalability of spin locks

Compare the variation in time per protected operation vs the number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and explain why they have this shape. Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

At 1 thread, Spin locks have a lower cost per protected operation compared to using a Mutex. However, at 2 threads, Spin locks cause the cost per protected operation to become higher than the cost per protected operation for using a Mutex. The costs per protected operation for both Spin locks and Mutexes constantly increase as the number of threads increases.

The cost per protected operation as the number of threads increases while using Spin locks is a graph that constantly increases as the number of threads increases; however, the shape is not a line because at higher thread counts, there are areas of sudden increases due to the higher possibility of threads attempting to enter the critical section during the same time period. Thus, the shape resembles that of a line with a positive slope, with some zig zag areas of higher slopes due to thread congestion.
The cost per protected operation as the number of threads increases while using Mutexes is also a relatively linear graph with a positive slope. However, this graph more closely resembles a straight line with less drastic slope changes, as the use of Mutexes is more consistent than Spin locking, where a thread could potentially spin for a long time. Thus, the shape resembles that of a line with a positive slope, with some minor zig zag areas that appear less drastic compared to the shape of the Spin lock graph.

This comparison illustrates that Spin locking is quite effective for the 1 thread case; because there is only one thread, there is no waiting time, so the overhead of spinning is not incurred. We can see from the graph that the simplicity of Spin locking for 1 thread is less costly than the extra overhead of locking and unlocking a Mutex. However, the graphs also show us that as we increase the number of threads, the protected operation cost of Spin locking skyrockets, while the protected operation cost of using Mutexes increases at a slower rate. This happens because the probability of a multiple threads trying to access the same critical section increases, which means threads will wait until it is their turn. In the case of Spin locking, there will be a lot of spinning happening, which is costly because CPU cycles are wasted, adding to the protected operation cost. Likewise, in the case of Mutexes, the overhead of locking and unlocking a Mutex is less costly than spinning, but the overall cost is still increasing, just at a smaller slope.

Research Links:
Pthread: https://computing.llnl.gov/tutorials/pthreads/
Atomic Builtins: http://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html
Timing with clock_gettime: http://man7.org/linux/man-pages/man2/clock_gettime.2.html
Double Linked Lists: https://www.tutorialspoint.com/data_structures_algorithms/doubly_linked_list_algorithm.htm
